# RAG Pipeline with LangChain, Gemini, and ChromaDB

This project implements a **Retrieval-Augmented Generation (RAG) pipeline** using **LangChain** to connect a **Google Gemini-2.5-Flash** Large Language Model (LLM) with a custom knowledge base. It uses **ChromaDB** for vector storage and **HuggingFace** embeddings to turn proprietary documents into searchable vectors.

## ğŸ§ What Is Happening?

The script sets up and executes a RAG-based question-answering system through the following stages:

1.  **Configuration and Setup:** Loads the `GOOGLE_API_KEY` from a **`.env`** file and defines the low-cost LLM (`gemini-2.5-flash`) and the local vector database name (`vector_db`).
2.  **Data Loading and Chunking:** It loads all **Markdown (`.md`)** files from the `knowledge-base/` directory, tagging them with their folder name as the `doc_type`. These documents are then split into smaller, overlapping **chunks** (size 1000, overlap 200) for effective retrieval.
3.  **Embedding and Vector Store:** The **HuggingFace `all-MiniLM-L6-v2`** model generates dense vector representations (embeddings) for each chunk. A new **Chroma** vector store is created from these chunks, replacing any existing collection.
4.  **LLM and Memory Initialization:** A **`ChatGoogleGenerativeAI`** instance using `gemini-2.5-flash` is initialized. **`ConversationBufferMemory`** is set up to maintain the chat history and context throughout the conversation.
5.  **RAG Chain Setup:** The vector store is used as a **retriever**. A **`ConversationalRetrievalChain`** is built, linking the Gemini LLM, the retriever, and the memory. This structure allows the LLM to efficiently retrieve relevant context *before* generating an answer.

---

## ğŸ› ï¸ How to Set Up and Run

Follow these steps to replicate and run the RAG pipeline.

### Prerequisites

* Python (3.x recommended)
* A `GOOGLE_API_KEY` for the Gemini API.

### 1. Environment and Dependencies

1.  **Install Dependencies:** Install the required Python packages.

    ```bash
    pip install python-dotenv google-genai langchain langchain-google-genai langchain-chroma langchain-huggingface
    ```

2.  **Set API Key:** Create a file named **`.env`** in the root directory and add your Google API key.

    ```dotenv
    # .env file
    GOOGLE_API_KEY="YOUR_API_KEY_HERE"
    ```

### 2. Prepare the Knowledge Base

1.  Create a root folder named `knowledge-base/`.
2.  Inside `knowledge-base/`, create subfolders (e.g., `policies`, `marketing`) to categorize your documents.
3.  Place your source documents (must be in **Markdown `.md` format**) into these subfolders.

    ```
    .
    â”œâ”€â”€ .env
    â”œâ”€â”€ knowledge-base/
    â”‚   â”œâ”€â”€ policies/
    â”‚   â”‚   â””â”€â”€ policy_details.md
    â”‚   â””â”€â”€ marketing/
    â”‚       â””â”€â”€ insurellm_description.md
    â””â”€â”€ rag_script.py (Your code)
    ```

### 3. Run the Script

Execute the Python file (e.g., `rag_script.py`):

```bash
python rag_script.py
```

### ğŸ–¥ï¸ Example Output
This section shows the console log and the final answer generated by the RAG pipeline for the query

